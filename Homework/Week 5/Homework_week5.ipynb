{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a56da6ac-ac5c-4092-bd5a-aa7ddc3393b9",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "\n",
    "Install Spark and PySpark\n",
    "\n",
    "Install Spark\n",
    "Run PySpark\n",
    "Create a local spark session\n",
    "Execute spark.version.\n",
    "What's the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "395059f4-05e1-49b8-b3f6-d0ce7a71c928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/02 19:29:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f10d16c-df1d-4a83-8186-2fd5d64e37ed",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "\n",
    "FHV October 2019\n",
    "\n",
    "Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\n",
    "\n",
    "Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8315fa-6d1f-4998-adf5-525576a7e9dc",
   "metadata": {},
   "source": [
    "We transform into a column schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a92d625c-1ed1-49d1-9d9e-c610560d2daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "fhv_schema = types.StructType([\n",
    "    types.StructField(\"dispatching_base_num\", types.StringType(), True),\n",
    "    types.StructField(\"pickup_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"dropoff_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"PULocationID\", types.LongType(), True),\n",
    "    types.StructField(\"DOLocationID\", types.LongType(), True),\n",
    "    types.StructField(\"SR_Flag\", types.StringType(), True),\n",
    "    types.StructField(\"Affiliated_base_number\", types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b70d87-392d-48cb-b415-b1507a65897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(fhv_schema) \\\n",
    "    .csv(\"fhv_tripdata_2019-10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e9a37f6-032c-473b-a57c-fe7a85b0d540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   null|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   null|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|         264|         264|   null|                B00014|\n",
      "|     B00021         |2019-10-01 00:00:48|2019-10-01 00:07:12|         129|         129|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:47:23|2019-10-01 00:53:25|          57|          57|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:10:06|2019-10-01 00:19:50|         173|         173|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:51:37|2019-10-01 01:06:14|         226|         226|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:28:23|2019-10-01 00:34:33|          56|          56|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:31:17|2019-10-01 00:51:52|          82|          82|   null|       B00021         |\n",
      "|              B00037|2019-10-01 00:07:41|2019-10-01 00:15:23|         264|          71|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:13:38|2019-10-01 00:25:51|         264|          39|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:42:40|2019-10-01 00:53:47|         264|         188|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:58:46|2019-10-01 01:10:11|         264|          91|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:09:49|2019-10-01 00:14:37|         264|          71|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:22:35|2019-10-01 00:36:53|         264|          35|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:54:27|2019-10-01 01:03:37|         264|          61|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:08:12|2019-10-01 00:28:47|         264|         198|   null|                B00037|\n",
      "|              B00053|2019-10-01 00:05:24|2019-10-01 00:53:03|         264|         264|   null|                  #N/A|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3dd9b8-87b1-4fea-b47d-be54aba3a56f",
   "metadata": {},
   "source": [
    "Write a parquet partioned in 6 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4d5190d-d4b6-4d86-8c8e-34adf03d632d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhv \\\n",
    "    .repartition(6) \\\n",
    "    .write.parquet('data/pq/fhvhv/2019/10/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7e637f-644c-48b7-8614-87b5516d9961",
   "metadata": {},
   "source": [
    "To check the size of the parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c738626-54e4-4faa-a3e9-3a4c4fa81a93",
   "metadata": {},
   "source": [
    "ls -lh\n",
    "6.2M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56d5b627-c55f-4ee9-a231-c63f539693a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhvh_oct = spark.read.parquet('data/pq/fhvhv/2019/10/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1180d70-1050-438c-b806-8652b634478f",
   "metadata": {},
   "source": [
    "## Question 3:\n",
    "\n",
    "Count records\n",
    "\n",
    "How many taxi trips were there on the 15th of October?\n",
    "\n",
    "Consider only trips that started on the 15th of October."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2925052f-4f9c-4bb2-bf1d-25475946507b",
   "metadata": {},
   "source": [
    "## SPARK SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f907e45-7b8e-4d99-94bb-c564e2dcf90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|number_records|\n",
      "+--------------+\n",
      "|         62610|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhv.createOrReplaceTempView(\"df_fhv\")\n",
    "\n",
    "df_sql = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    df_fhv\n",
    "WHERE date_trunc('day', pickup_datetime) == \"2019-10-15\"\n",
    "\"\"\")\n",
    "\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a2d1d-2465-4dac-bfef-b6b00e2465ea",
   "metadata": {},
   "source": [
    "## PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "281a623e-ede4-4990-8ff9-01de4f467141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_fhv_15_october = df_fhv \\\n",
    "    .filter(F.date_trunc(\"day\", \"pickup_datetime\") == \"2019-10-15\") \n",
    "\n",
    "df_fhv_15_october.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c5fe7b-77ef-4d79-a666-dead93933276",
   "metadata": {},
   "source": [
    "## Question 4:\n",
    "\n",
    "Longest trip for each day\n",
    "\n",
    "What is the length of the longest trip in the dataset in hours?\n",
    "\n",
    "631,152.50 Hours\n",
    "243.44 Hours\n",
    "7.68 Hours\n",
    "3.32 Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91ac51ea-83f4-484e-98fb-d70a3c4fa92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+\n",
      "|    pickup_datetime|   dropoff_datetime|              diff|\n",
      "+-------------------+-------------------+------------------+\n",
      "|2019-10-28 09:00:00|2091-10-28 09:30:00|          631152.5|\n",
      "|2019-10-11 18:00:00|2091-10-11 18:30:00|          631152.5|\n",
      "|2019-10-31 23:46:33|2029-11-01 00:13:00| 87672.44083333333|\n",
      "|2019-10-01 21:43:42|2027-10-01 21:45:23| 70128.02805555555|\n",
      "|2019-10-17 14:00:00|2020-10-18 00:00:00|            8794.0|\n",
      "|2019-10-26 21:26:00|2020-10-26 21:36:00| 8784.166666666666|\n",
      "|2019-10-30 12:30:04|2019-12-30 13:02:08|1464.5344444444445|\n",
      "|2019-10-25 07:04:57|2019-12-08 07:54:33|1056.8266666666666|\n",
      "|2019-10-25 07:04:57|2019-12-08 07:21:11|1056.2705555555556|\n",
      "|2019-10-01 13:47:17|2019-11-03 15:20:28| 793.5530555555556|\n",
      "|2019-10-01 07:21:12|2019-11-03 08:44:21| 793.3858333333334|\n",
      "|2019-10-01 13:41:00|2019-11-03 14:58:51|          793.2975|\n",
      "|2019-10-01 18:43:20|2019-11-03 19:43:13| 792.9980555555555|\n",
      "|2019-10-01 18:43:46|2019-11-03 19:43:04| 792.9883333333333|\n",
      "|2019-10-01 07:07:09|2019-11-03 07:58:46| 792.8602777777778|\n",
      "|2019-10-01 14:49:28|2019-11-03 15:38:07| 792.8108333333333|\n",
      "|2019-10-01 05:36:30|2019-11-03 06:23:36|           792.785|\n",
      "|2019-10-01 15:02:55|2019-11-03 15:49:05| 792.7694444444444|\n",
      "|2019-10-01 06:08:01|2019-11-03 06:53:15| 792.7538888888889|\n",
      "|2019-10-01 06:41:17|2019-11-03 07:26:04| 792.7463888888889|\n",
      "+-------------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhv_longest = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    pickup_datetime, dropoff_datetime,\n",
    "    (unix_timestamp(dropoff_datetime)-unix_timestamp(pickup_datetime))/(3600) as diff \n",
    "FROM\n",
    "    df_fhv\n",
    "    order by 3 desc\n",
    "\"\"\")\n",
    "\n",
    "df_fhv_longest.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e259915-8761-4d2c-bb73-96dd2cf12338",
   "metadata": {},
   "source": [
    "## Using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbbc0f4d-7370-4a83-a9a4-b43e1ace13a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[dispatching_base_num: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: bigint, DOLocationID: bigint, SR_Flag: string, Affiliated_base_number: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68d78958-1199-4946-a355-366a6d9661ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|              diff|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+------------------+\n",
      "|              B02832|2019-10-28 09:00:00|2091-10-28 09:30:00|         264|         264|   null|                B02832|          631152.5|\n",
      "|              B02832|2019-10-11 18:00:00|2091-10-11 18:30:00|         264|         264|   null|                B02832|          631152.5|\n",
      "|              B02416|2019-10-31 23:46:33|2029-11-01 00:13:00|        null|        null|   null|                B02416| 87672.44083333333|\n",
      "|     B00746         |2019-10-01 21:43:42|2027-10-01 21:45:23|         159|         264|   null|       B00746         | 70128.02805555555|\n",
      "|              B02921|2019-10-17 14:00:00|2020-10-18 00:00:00|        null|        null|   null|                B03037|            8794.0|\n",
      "|              B03110|2019-10-26 21:26:00|2020-10-26 21:36:00|         264|         264|   null|                B03110| 8784.166666666666|\n",
      "|              B03080|2019-10-30 12:30:04|2019-12-30 13:02:08|         264|          50|   null|                B02883|1464.5344444444445|\n",
      "|     B03084         |2019-10-25 07:04:57|2019-12-08 07:54:33|         168|         235|   null|                B02765|1056.8266666666666|\n",
      "|     B03084         |2019-10-25 07:04:57|2019-12-08 07:21:11|         168|         235|   null|                B02765|1056.2705555555556|\n",
      "|              B01452|2019-10-01 13:47:17|2019-11-03 15:20:28|          44|         214|   null|                B01452| 793.5530555555556|\n",
      "|              B01452|2019-10-01 07:21:12|2019-11-03 08:44:21|           5|           6|   null|                B01452| 793.3858333333334|\n",
      "|              B01452|2019-10-01 13:41:00|2019-11-03 14:58:51|         206|         172|   null|                B01452|          793.2975|\n",
      "|              B01452|2019-10-01 18:43:20|2019-11-03 19:43:13|          23|           5|   null|                B01452| 792.9980555555555|\n",
      "|              B01452|2019-10-01 18:43:46|2019-11-03 19:43:04|         251|          44|   null|                B01452| 792.9883333333333|\n",
      "|              B01452|2019-10-01 07:07:09|2019-11-03 07:58:46|         204|          23|   null|                B01452| 792.8602777777778|\n",
      "|              B01452|2019-10-01 14:49:28|2019-11-03 15:38:07|         214|         156|   null|                B01452| 792.8108333333333|\n",
      "|              B01452|2019-10-01 05:36:30|2019-11-03 06:23:36|         214|          84|   null|                B01452|           792.785|\n",
      "|              B00972|2019-10-01 15:02:55|2019-11-03 15:49:05|         204|           1|   null|                B00972| 792.7694444444444|\n",
      "|              B00972|2019-10-01 06:08:01|2019-11-03 06:53:15|         156|         204|   null|                B00972| 792.7538888888889|\n",
      "|              B01452|2019-10-01 06:41:17|2019-11-03 07:26:04|          44|          23|   null|                B01452| 792.7463888888889|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_fhv_2019_october = df_fhv \\\n",
    "    .withColumn(\"diff\", (unix_timestamp(\"dropoff_datetime\")-unix_timestamp(\"pickup_datetime\"))/3600) \\\n",
    "    .orderBy(col(\"diff\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac28efb-9b1b-476a-835c-d657d87b5f13",
   "metadata": {},
   "source": [
    "## Question 6:\n",
    "\n",
    "Least frequent pickup location zone\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark\n",
    "Zone Data\n",
    "\n",
    "Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?\n",
    "\n",
    "East Chelsea\n",
    "Jamaica Bay\n",
    "Union Sq\n",
    "Crown Heights North"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a53d6ca0-07d9-496d-8b3e-3784e4afdda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"taxi_zone_lookup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "636fbcd7-fa82-4202-a7ab-94b42293de81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[LocationID: string, Borough: string, Zone: string, service_zone: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a50cd63-8aa6-4d4c-8617-482a34c03dcc",
   "metadata": {},
   "source": [
    "Rename the locationID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "663a2d20-c940-4209-bb7e-fe494d46e58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = df_zones.withColumnRenamed('LocationID', 'PULocationID') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1b5001-9d71-4769-bda9-5b15a4eb590f",
   "metadata": {},
   "source": [
    "Join both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ace91a47-0e42-49d3-b22a-65001ffaa8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = df_fhv.join(df_zones.select(\"PULocationID\",\"Zone\"), on=['PULocationID'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c019d6-4cce-43f8-a452-3782141f9651",
   "metadata": {},
   "source": [
    "Create temp table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c24c644a-96f6-4e5a-b273-f819892232a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join.createOrReplaceTempView(\"df_join\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb77bc-cf1a-4abe-9b9f-de8a9c9c3cec",
   "metadata": {},
   "source": [
    "Using SPARK SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f499d520-49e4-432b-8740-bc9f4e1fd473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|count|                zone|\n",
      "+-----+--------------------+\n",
      "|    1|         Jamaica Bay|\n",
      "|    2|Governor's Island...|\n",
      "|    5| Green-Wood Cemetery|\n",
      "|    8|       Broad Channel|\n",
      "|   14|     Highbridge Park|\n",
      "|   15|        Battery Park|\n",
      "|   23|Saint Michaels Ce...|\n",
      "|   25|Breezy Point/Fort...|\n",
      "|   26|Marine Park/Floyd...|\n",
      "|   29|        Astoria Park|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_less_fzones = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    count(*) as count, zone\n",
    "FROM\n",
    "    df_join\n",
    "GROUP BY zone\n",
    "ORDER BY count ASC\n",
    "\"\"\")\n",
    "df_less_fzones.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd65b983-5d64-4351-bafd-9ae9c8d7b0cd",
   "metadata": {},
   "source": [
    "Using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50930bb3-e682-4b33-8602-67bb7c2733ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                zone|count|\n",
      "+--------------------+-----+\n",
      "|         Jamaica Bay|    1|\n",
      "|Governor's Island...|    2|\n",
      "| Green-Wood Cemetery|    5|\n",
      "|       Broad Channel|    8|\n",
      "|     Highbridge Park|   14|\n",
      "|        Battery Park|   15|\n",
      "|Saint Michaels Ce...|   23|\n",
      "|Breezy Point/Fort...|   25|\n",
      "|Marine Park/Floyd...|   26|\n",
      "|        Astoria Park|   29|\n",
      "|    Inwood Hill Park|   39|\n",
      "|       Willets Point|   47|\n",
      "|Forest Park/Highl...|   53|\n",
      "|  Brooklyn Navy Yard|   57|\n",
      "|        Crotona Park|   62|\n",
      "|        Country Club|   77|\n",
      "|     Freshkills Park|   89|\n",
      "|       Prospect Park|   98|\n",
      "|     Columbia Street|  105|\n",
      "|  South Williamsburg|  110|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "less_f_zones_pysparK = df_join \\\n",
    ".groupBy(\"zone\").count() \\\n",
    ".sort(col(\"count\").asc()) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396907aa-0a90-4139-ac6b-3ec3b16d06c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
